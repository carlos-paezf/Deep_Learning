{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "P1T5_S4_Multinomial_Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCxJ7Ha8T8IM"
      },
      "source": [
        "# Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JXWYyx4T_9Q"
      },
      "source": [
        "The total credits in this document are for: [Machine Learning Mastery: Multinomial Logistic Regression](https://machinelearningmastery.com/multinomial-logistic-regression-with-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-7TVLN9Uwno"
      },
      "source": [
        "## Concept"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmoWFWFwUyb8"
      },
      "source": [
        "Multinomial logistic regression is an extension of logistic regression that adds native support for multi-class classification problems. Logistic regression, by default, is limited to two class classification problems. Some extensions like one-vs-rest can allow logistic regression to be used for multi-class classification problems, although they require that the classification problem first be transformed into multiple binary classification problems.  \n",
        "Instead, the multinomial logistic regression algorithm is an extension to the logistic regression model thah involves changing the loss function to cross-entropy loss an d predict probability distribution to a multinomial probability distribution to natively support multi-class classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UJgcD0uy-_l"
      },
      "source": [
        "Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FCLRHRczBSA"
      },
      "source": [
        "First, we will define a synthetic multi-class classification dataset to use as the basis of the investigation. This is a generic dataset that you can easily replace with your own loaded dataset later.\n",
        "\n",
        "The make_classification() function can be used to generate a dataset with a given number of rows, columns, and classes. In this case, we will generate a dataset with 1,000 rows, 10 input variables or columns, and 3 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJRkVIZaTrz-"
      },
      "source": [
        "from collections import Counter\n",
        "from sklearn.datasets import make_classification"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZlTd3aKzwwc",
        "outputId": "1014b581-5d3a-4393-d94c-dde6f4b3e663"
      },
      "source": [
        "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, n_classes=3, random_state=1)\n",
        "print(X.shape, y.shape)\n",
        "print(Counter(y))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 10) (1000,)\n",
            "Counter({1: 334, 2: 334, 0: 332})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bsv1qw123Ao"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGLe4mjy3P5E",
        "outputId": "8bc408ed-377a-4817-f513-b345ae3f1644"
      },
      "source": [
        "from numpy import mean, std\n",
        "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
        "\n",
        "# Define the model evaluation procedure\n",
        "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "\n",
        "# Evaluate the model and collect the scores\n",
        "n_scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "\n",
        "# Report the model performance\n",
        "print('Mean Accuaracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mean Accuaracy: 0.681 (0.042)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjToNDTQ5adC"
      },
      "source": [
        "In this case, we can see that the multinonial logistic regression model with default penalty achieved a mean classification accuaracy of about 68.1% on our synthetic classification dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BT0nreUa4xCk",
        "outputId": "cceaaf87-1242-4914-ec87-8943aad65cc1"
      },
      "source": [
        "# Fit the model on the whole dataset\n",
        "model.fit(X, y)\n",
        "\n",
        "# Define a single row of input data\n",
        "row = [1.89149379, -0.39847585, 1.63856893, 0.01647165, 1.51892395, -3.52651223, 1.80998823, 0.58810926, -0.02542177, -0.52835426]\n",
        "\n",
        "# Predict the class label\n",
        "yhat = model.predict([row])\n",
        "\n",
        "# Summarize the predicted class\n",
        "print('Predicted class: %d' % yhat[0])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted class: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gw3nvz-8B8A_"
      },
      "source": [
        "In this case, we can see that the model predicted the class \"1\" for the single row of data.  \n",
        "A benefit of multinomial logistic regression is that it can predict calibrated probabilites across all known class labels in the dataset. This can be achieved by calling the predict_proba() function on the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wzj5H9-0DgZ_",
        "outputId": "fd82e038-5684-4950-cf1e-edc4b65a8028"
      },
      "source": [
        "yhat = model.predict_proba([row])\n",
        "\n",
        "# Summarize the predicted probabilities\n",
        "print('Predicted Probabilities: %s' % yhat[0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted Probabilities: [0.16470456 0.50297138 0.33232406]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7x5lcLgGRdb"
      },
      "source": [
        "In this case, we can see that class 1 (e.g. the array index is mapped to the class integer value) has the largest predicted probability with about 0.50."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hY4qVkjsG1dp"
      },
      "source": [
        "# Tune Penalty for Multinomial Logisitic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWqNn999G6xg"
      },
      "source": [
        "An important hyperparameter to tune for multinomial logistic regression is the penalty term.\n",
        "\n",
        "This term imposes pressure on the model to seek smaller model weights. This is achieved by adding a weighted sum of the model coefficients to the loss function, encouraging the model to reduce the size of the weights along with the error while fitting the model.\n",
        "\n",
        "A popular type of penalty is the L2 penalty that adds the (weighted) sum of the squared coefficients to the loss function. A weighting of the coefficients can be used that reduces the strength of the penalty from full penalty to a very slight penalty.\n",
        "\n",
        "By default, the LogisticRegression class uses the L2 penalty with a weighting of coefficients set to 1.0. The type of penalty can be set via the “penalty” argument with values of “l1“, “l2“, “elasticnet” (e.g. both), although not all solvers support all penalty types. The weighting of the coefficients in the penalty can be set via the “C” argument.\n",
        "\n",
        "```python\n",
        "LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=1.0)\n",
        "```\n",
        "\n",
        "- C close to 1.0: Light penalty.\n",
        "- C close to 0.0: Strong penalty.  \n",
        "\n",
        "The penalty can be disabled by setting the “penalty” argument to the string “none“.\n",
        "\n",
        "```python\n",
        "LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCU8NDo-IwQ0"
      },
      "source": [
        "Now that we are familiar with the penalty, let’s look at how we might explore the effect of different penalty values on the performance of the multinomial logistic regression model.\n",
        "\n",
        "It is common to test penalty values on a log scale in order to quickly discover the scale of penalty that works well for a model. Once found, further tuning at that scale may be beneficial.\n",
        "\n",
        "We will explore the L2 penalty with weighting values in the range from 0.0001 to 1.0 on a log scale, in addition to no penalty or 0.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "t0a2o0OCG8Ni",
        "outputId": "92e03047-f4a2-4b25-cd11-fedabe118099"
      },
      "source": [
        "from numpy import mean, std\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def get_dataset():\n",
        "    X, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=1, n_classes=3)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def get_models():\n",
        "    models = dict()\n",
        "    for p in [0.0, 0.0001, 0.001, 0.01, 0.1, 1.0]:\n",
        "        key = '%.4f' % p\n",
        "        if p == 0.0:\n",
        "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='none')\n",
        "        else:\n",
        "            models[key] = LogisticRegression(multi_class='multinomial', solver='lbfgs', penalty='l2', C=p)\n",
        "    return models\n",
        "\n",
        "\n",
        "def evaluate_model(model, X, y):\n",
        "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
        "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
        "    return scores\n",
        "\n",
        "\n",
        "X, y = get_dataset()\n",
        "models = get_models()\n",
        "results, names = list(), list()\n",
        "\n",
        "for name, model in models.items():\n",
        "    scores = evaluate_model(model, X, y)\n",
        "    results.append(scores)\n",
        "    names.append(name)\n",
        "    print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
        "\n",
        "plt.boxplot(results, labels=names, showmeans=True)\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ">0.0000 0.777 (0.037)\n",
            ">0.0001 0.683 (0.049)\n",
            ">0.0010 0.762 (0.044)\n",
            ">0.0100 0.775 (0.040)\n",
            ">0.1000 0.774 (0.038)\n",
            ">1.0000 0.777 (0.037)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmUD8n-_OBOc"
      },
      "source": [
        "In this case, we can see that a C value of 1.0 has the best score of about 77.7%, which is the same using no penalty that achivieves the same score.  \n",
        "\n",
        "A box and whisker plot is created for the accuracy scores for each configuration and all plots are shown side by side on a figure on the same scale for direct comparison.\n",
        "\n",
        "In this case, we can see that the larger penalty we use on this dataset (i.e. the smaller the C value), the worse the performance of the model."
      ]
    }
  ]
}